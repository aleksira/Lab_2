{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Это код, который находит ссылки всех статей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-263-31407f72b55e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0marticles_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mart_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticles_url\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0marticle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-261-54e982b83b31>\u001b[0m in \u001b[0;36marticle\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0minf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"border-bottom: 1px solid silver;\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'№[\\d]+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[\\d]{1,2}\\s[а-яё]+\\s[\\d]{4}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "url = 'https://risk-inform.ru/arhive.html'\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "years = soup.find_all(class_='main_block')[1]\n",
    "url_years = years.find_all(href=True)\n",
    "years_url = []\n",
    "articles_url = []\n",
    "issues_url = []\n",
    "for elem in url_years:\n",
    "    year_href = elem['href']\n",
    "    url_year = 'https://risk-inform.ru/' + year_href\n",
    "    if ('arh_num' or 'board') not in url_year:\n",
    "        years_url.append(url_year)\n",
    "for url_year in years_url:\n",
    "    ry = requests.get(url_year)\n",
    "    soupy = BeautifulSoup(ry.content, 'html.parser')\n",
    "    issues = soupy.find_all('span', class_='A_black')\n",
    "    for iss in issues:\n",
    "        iss_href = iss.find(href=True)['href']\n",
    "        url_issue = 'https://risk-inform.ru/' + iss_href\n",
    "        ri = requests.get(url_issue)\n",
    "        soupi = BeautifulSoup(ri.content, 'html.parser')\n",
    "        urls_article = soupi.find_all('li', type='square')\n",
    "        for elem in urls_article:\n",
    "            art_href = elem.find(href=True)['href']\n",
    "            art_url = 'https://risk-inform.ru/' + art_href\n",
    "            articles_url.append(art_url)\n",
    "for url in articles_url:\n",
    "    article(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Это функция, которая достает данные из статьи и кладет в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    inf = soup.find('table', style=\"border-bottom: 1px solid silver;\").get_text()\n",
    "    num = ''.join(re.findall('№[\\d]+', inf))\n",
    "    date = ''.join(re.findall('[\\d]{1,2}\\s[а-яё]+\\s[\\d]{4}', inf))\n",
    "    article = soup.find(class_='statia')\n",
    "    if article.find(\"p\", align='right'):\n",
    "        author = article.find(\"p\", align='right').get_text()\n",
    "    else:\n",
    "        author = 'Автор не указан'\n",
    "    if soup.find(class_='storytitle') != None:\n",
    "        title = soup.find(class_='storytitle').get_text()\n",
    "    elif soup.find(class_='rubrika') != None:\n",
    "        title = soup.find(class_='rubrika').get_text()\n",
    "        title = re.sub('\\xa0\\xa0\\xa0\\xa0', '', title)\n",
    "    text = article.get_text().replace(title, '').replace(author, '').strip()\n",
    "    statia = '=====\\n' + url + '\\n' + 'Газета «РИСК» ' + num + '\\n' + date + '\\n' + author + '\\n' + title + '\\n' + text + '\\n'\n",
    "    with open('placefornews.txt', 'a', encoding='utf-8') as file:\n",
    "        file.write(statia)\n",
    "    #data_graph = WordAfterWord(text, 4)\n",
    "    #assymetry(data_graph, 3)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "article('https://risk-inform.ru/text/27/prazd.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### На этой статье я смотрела, как создается граф и как в нем ищутся нужные пары слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['из, переселенцев',\n",
       " 'из, из',\n",
       " 'с, наконец',\n",
       " 'в, млн',\n",
       " 'в, россии',\n",
       " 'в, непростой',\n",
       " 'в, но',\n",
       " 'в, чём',\n",
       " 'в, поддержать',\n",
       " 'в, этого',\n",
       " 'в, вынуждены',\n",
       " 'в, туве',\n",
       " 'в, больше',\n",
       " 'в, парламенте',\n",
       " 'в, в',\n",
       " 'в, его',\n",
       " 'в, и',\n",
       " 'в, зачем',\n",
       " 'в, эксплуатацию',\n",
       " 'в, кто',\n",
       " 'россии, в',\n",
       " 'туве, в',\n",
       " 'на, курорт',\n",
       " 'на, деньги',\n",
       " 'на, озеро',\n",
       " 'на, на',\n",
       " 'на, то',\n",
       " 'и, курорт',\n",
       " 'и, даже',\n",
       " 'и, что',\n",
       " 'и, другие',\n",
       " 'и, объекты',\n",
       " 'и, это',\n",
       " 'и, в',\n",
       " 'и, его',\n",
       " 'и, нерентабельным',\n",
       " 'и, и',\n",
       " 'и, караоол',\n",
       " 'и, нищета',\n",
       " 'и, кызыла',\n",
       " 'и, они',\n",
       " 'и, уже',\n",
       " 'и, наркоманами',\n",
       " 'и, их',\n",
       " 'и, путин',\n",
       " 'и, тут',\n",
       " 'и, шойгу',\n",
       " 'и, но',\n",
       " 'и, ещё',\n",
       " 'и, жители',\n",
       " 'комплекс, замкнутым',\n",
       " 'замкнутым, комплекс',\n",
       " 'курорт, сейчас',\n",
       " 'курорт, на',\n",
       " 'курорт, и',\n",
       " 'курорт, он',\n",
       " 'уже, и',\n",
       " 'даже, и',\n",
       " 'не, стало',\n",
       " 'не, не',\n",
       " 'не, но',\n",
       " 'не, караоолу',\n",
       " 'не, сам',\n",
       " 'не, из',\n",
       " 'что, и',\n",
       " 'что, давно',\n",
       " 'что, они',\n",
       " 'будет, но',\n",
       " 'сам, не',\n",
       " 'млн, в',\n",
       " 'чтобы, стала',\n",
       " 'сейчас, курорт',\n",
       " 'он, курорт',\n",
       " 'один, спальный',\n",
       " 'один, остался',\n",
       " 'корпус, двухэтажный',\n",
       " 'ещё, и',\n",
       " 'спальный, один',\n",
       " 'двухэтажный, корпус',\n",
       " 'другие, и',\n",
       " 'объекты, и',\n",
       " 'это, жители',\n",
       " 'это, и',\n",
       " 'был, но',\n",
       " 'но, не',\n",
       " 'но, был',\n",
       " 'но, и',\n",
       " 'но, будет',\n",
       " 'но, в',\n",
       " 'тут, и',\n",
       " 'за, остался',\n",
       " 'их, и',\n",
       " 'шойгу, и',\n",
       " 'его, и',\n",
       " 'его, в',\n",
       " 'караоолу, не',\n",
       " 'бюджетных, в',\n",
       " 'деньги, на',\n",
       " 'к, тувинского',\n",
       " 'ответственности, телеканала',\n",
       " 'стало, не',\n",
       " 'непростой, в',\n",
       " 'года, спустя',\n",
       " 'года, три',\n",
       " 'больше, в',\n",
       " 'караоол, и',\n",
       " 'кто, побогаче',\n",
       " 'кто, кто',\n",
       " 'кто, в',\n",
       " 'побогаче, кто',\n",
       " 'озеро, на',\n",
       " 'нерентабельным, и',\n",
       " 'остался, за',\n",
       " 'остался, один',\n",
       " 'то, на',\n",
       " 'чём, в',\n",
       " 'поддержать, в',\n",
       " 'стала, чтобы',\n",
       " 'нищета, и',\n",
       " 'кызыла, и',\n",
       " 'этого, в',\n",
       " 'вынуждены, в',\n",
       " 'спустя, года',\n",
       " 'три, года',\n",
       " 'они, и',\n",
       " 'они, что',\n",
       " 'жители, это',\n",
       " 'жители, и',\n",
       " 'давно, что',\n",
       " 'наконец, с',\n",
       " 'переселенцев, из',\n",
       " 'наркоманами, и',\n",
       " 'парламенте, в',\n",
       " 'тувинского, к',\n",
       " 'телеканала, ответственности',\n",
       " 'путин, и',\n",
       " 'зачем, в',\n",
       " 'эксплуатацию, в']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('https://risk-inform.ru/article_9013.html')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "inf = soup.find('table', style=\"border-bottom: 1px solid silver;\").get_text()\n",
    "num = ''.join(re.findall('№[\\d]+', inf))\n",
    "date = ''.join(re.findall('[\\d]{2}\\s[а-яё]+\\s[\\d]{4}', inf))\n",
    "article = soup.find(class_='statia')\n",
    "if article.find(\"p\", align='right'):\n",
    "    author = article.find(\"p\", align='right').get_text()\n",
    "else:\n",
    "    author = 'Автор не указан'\n",
    "if soup.find(class_='storytitle') != None:\n",
    "    title = soup.find(class_='storytitle').get_text()\n",
    "elif soup.find(class_='rubrika') != None:\n",
    "    title = soup.find(class_='rubrika').get_text()\n",
    "    title = re.sub('\\xa0\\xa0\\xa0\\xa0', '', title)\n",
    "text = article.get_text().replace(title, '').replace(author, '')\n",
    "data_graph = WordAfterWord(text, 3)\n",
    "#data_graph\n",
    "assymetry(data_graph, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построить граф связей слов, в котором слово представляет собой вершину, \n",
    "а взвешенная дуга показывает сколько раз слово в первой вершине упоминалось после\n",
    "слова во второй вершине в окне размера N. \n",
    "Найти все пары слов, для которых наблюдается несоблюдение симметрии \n",
    "в M раз (отношение частоты дуги 1->2 к частоте дуги 2->1 больше М)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Это функция для создания графа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordAfterWord(text, n):\n",
    "    punct = ['«', '»']\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    for el in punct:\n",
    "        text = text.replace(el, '')\n",
    "    text = re.sub('\\s\\–\\s', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    data = defaultdict(int)\n",
    "    for i in range(len(words) - (n-1)):\n",
    "        word1 = words[i].lower()\n",
    "        other_words = words[i:(n+i)]\n",
    "        group = ' '.join(other_words)\n",
    "        data[group] += 1\n",
    "    data_graph = defaultdict(lambda: None)\n",
    "    for word in list(data.keys()):\n",
    "        word1, word2 = word.split()[0], word.split()[1]\n",
    "        data_graph[word1] = defaultdict(int)\n",
    "        data_graph[word2] = defaultdict(int)\n",
    "    for group, num in data.items():\n",
    "        ws = group.split()\n",
    "        for j in range(len(ws)-1):\n",
    "            data_graph[ws[0]][ws[j+1]] = num\n",
    "        for k in range(len(ws)-2):\n",
    "            data_graph[ws[1]][ws[k+2]] = num\n",
    "    return data_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Это функция для поиска пар в графе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assymetry(data_graph, m):\n",
    "    assymetrical_pairs = []\n",
    "    for word in data_graph:\n",
    "        for word1 in data_graph[word]:\n",
    "            if word1 in data_graph:\n",
    "                if data_graph[word1][word] != 0:\n",
    "                    if (data_graph[word][word1]/data_graph[word1][word]) >= m:\n",
    "                        assymetrical_pairs.append(word + ', ' + word1)\n",
    "    return assymetrical_pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
